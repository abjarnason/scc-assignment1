Question 2. b)

Please add your run log along with commands to below in this file.
===================================================================


Warning: $HADOOP_HOME is deprecated.

14/11/03 14:41:10 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
14/11/03 14:41:11 INFO input.FileInputFormat: Total input paths to process : 1
14/11/03 14:41:11 INFO util.NativeCodeLoader: Loaded the native-hadoop library
14/11/03 14:41:11 WARN snappy.LoadSnappy: Snappy native library not loaded
14/11/03 14:41:12 INFO mapred.JobClient: Running job: job_201411031420_0003
14/11/03 14:41:13 INFO mapred.JobClient:  map 0% reduce 0%
14/11/03 14:41:35 INFO mapred.JobClient:  map 17% reduce 0%
14/11/03 14:41:38 INFO mapred.JobClient:  map 100% reduce 0%
14/11/03 14:41:52 INFO mapred.JobClient:  map 100% reduce 33%
14/11/03 14:41:55 INFO mapred.JobClient:  map 100% reduce 66%
14/11/03 14:41:58 INFO mapred.JobClient:  map 100% reduce 72%
14/11/03 14:42:02 INFO mapred.JobClient:  map 100% reduce 100%
14/11/03 14:42:10 INFO mapred.JobClient: Job complete: job_201411031420_0003
14/11/03 14:42:10 INFO mapred.JobClient: Counters: 29
14/11/03 14:42:10 INFO mapred.JobClient:   Job Counters 
14/11/03 14:42:10 INFO mapred.JobClient:     Launched reduce tasks=1
14/11/03 14:42:10 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=31665
14/11/03 14:42:10 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
14/11/03 14:42:10 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
14/11/03 14:42:10 INFO mapred.JobClient:     Launched map tasks=1
14/11/03 14:42:10 INFO mapred.JobClient:     Data-local map tasks=1
14/11/03 14:42:10 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=23354
14/11/03 14:42:10 INFO mapred.JobClient:   File Output Format Counters 
14/11/03 14:42:10 INFO mapred.JobClient:     Bytes Written=12217781
14/11/03 14:42:10 INFO mapred.JobClient:   FileSystemCounters
14/11/03 14:42:10 INFO mapred.JobClient:     FILE_BYTES_READ=9193604
14/11/03 14:42:10 INFO mapred.JobClient:     HDFS_BYTES_READ=12121912
14/11/03 14:42:10 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=18499559
14/11/03 14:42:10 INFO mapred.JobClient:     HDFS_BYTES_WRITTEN=12217781
14/11/03 14:42:10 INFO mapred.JobClient:   File Input Format Counters 
14/11/03 14:42:10 INFO mapred.JobClient:     Bytes Read=12121786
14/11/03 14:42:10 INFO mapred.JobClient:   Map-Reduce Framework
14/11/03 14:42:10 INFO mapred.JobClient:     Map output materialized bytes=9193604
14/11/03 14:42:10 INFO mapred.JobClient:     Map input records=146616
14/11/03 14:42:10 INFO mapred.JobClient:     Reduce shuffle bytes=9193604
14/11/03 14:42:10 INFO mapred.JobClient:     Spilled Records=293232
14/11/03 14:42:10 INFO mapred.JobClient:     Map output bytes=8896580
14/11/03 14:42:10 INFO mapred.JobClient:     Total committed heap usage (bytes)=185360384
14/11/03 14:42:10 INFO mapred.JobClient:     CPU time spent (ms)=19800
14/11/03 14:42:10 INFO mapred.JobClient:     Combine input records=0
14/11/03 14:42:10 INFO mapred.JobClient:     SPLIT_RAW_BYTES=126
14/11/03 14:42:10 INFO mapred.JobClient:     Reduce input records=146616
14/11/03 14:42:10 INFO mapred.JobClient:     Reduce input groups=109698
14/11/03 14:42:10 INFO mapred.JobClient:     Combine output records=0
14/11/03 14:42:10 INFO mapred.JobClient:     Physical memory (bytes) snapshot=310079488
14/11/03 14:42:10 INFO mapred.JobClient:     Reduce output records=109698
14/11/03 14:42:10 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=1501536256
14/11/03 14:42:10 INFO mapred.JobClient:     Map output records=146616

// finally we check the output

$ hadoop fs -copyToLocal output output
$ head -20 output/part-r-00000

{"author":" Antonio Zirardini","books":[{"book":"Degli antichi edifizi profani di Ravenna libri due"}]}
{"author":" E. M. Korkhmazyan","books":[{"book":"Armenian miniatures of the 13th and 14th centuries from the Matenadaran collection, Yerevan"}]}
{"author":" Edward J and G. Burrow","books":[{"book":"Little Lever Official Guide; published by Authority of the Little Lever Urban District Council"}]}
{"author":" I Uli i a Pavlovna Averkieva","books":[{"book":"Kwakiutl stringfigures"}]}
{"author":" Joseph L Ganey","books":[{"book":"Using terrestrial ecosystem survey data to identify potential habitat for the Mexican spotted owl on national forest system lands"}]}
{"author":" Keith Hemsley","books":[{"book":"The NCET primary data-logging project"}]}
{"author":" Leon Edel","books":[{"book":"Henry James The Treacherous years"}]}
{"author":" Paulo Ferreira","books":[{"book":"O Mensageiro \u2013 Vol. 1 \u2013 O Despertar para um Novo Mundo."}]}
{"author":"\"BB\"","books":[{"book":"Tides Ending"}]}
{"author":"\"BB,\"","books":[{"book":"Tide's ending"},{"book":"A summer on the Nene"},{"book":"Brendon chase"},{"book":"Ramblings of a sportsman-naturalist"}]}
{"author":"\"Creating a Universal Environment\" Conference (1991 Waterloo, Ont.)","books":[{"book":"Proceedings of \"Creating a Universal Environment\" Conference"}]}
{"author":"\"Edinburgh Evening News\"","books":[{"book":"Images of Edinburgh (Images of)"}]}
{"author":"\"Financial Times\"","books":[{"book":"World Desk Reference (World Atlas)"}]}
{"author":"\"Good Housekeeping\".","books":[{"book":"Using an ice cream maker"}]}
{"author":"\"Hebukesaier Menggu Zizhixian gai kuang\" bian xie zu","books":[{"book":"Hebukesaier Menggu Zizhixian gai kuang ="}]}
{"author":"\"Huan qiu qi ye jia \" za zhi she","books":[{"book":"Huan qiu qi ye jia"}]}
{"author":"\"J. J.\" \"Jones\"","books":[{"book":"Deirdre"}]}
{"author":"\"Organic Gardening and Farming\"","books":[{"book":"Calendar of Organic Gardening"}]}
{"author":"\"Penelope\".","books":[{"book":"Jacobean Crewel work and traditional designs"}]}
{"author":"\"Ren wu\" bian ji bu","books":[{"book":"Zuo ye chang feng yi zhi qin ="}]}

// in this output we can actually only see one example of more than one book by an author ("\"BB,\""). And then put it into the output-a/ directory

$ cp output/part-r-00000 output-a
Usage: jar {ctxui}[vfm0Me] [jar-file] [manifest-file] [entry-point] [-C dir] files ...
Options:
    -c  create new archive
    -t  list table of contents for archive
    -x  extract named (or all) files from archive
    -u  update existing archive
    -v  generate verbose output on standard output
    -f  specify archive file name
    -m  include manifest information from specified manifest file
    -e  specify application entry point for stand-alone application 
        bundled into an executable jar file
    -0  store only; use no ZIP compression
    -M  do not create a manifest file for the entries
    -i  generate index information for the specified jar files
    -C  change to the specified directory and include the following file
If any file is a directory then it is processed recursively.
The manifest file name, the archive file name and the entry point name are
specified in the same order as the 'm', 'f' and 'e' flags.

Example 1: to archive two class files into an archive called classes.jar: 
       jar cvf classes.jar Foo.class Bar.class 
Example 2: use an existing manifest file 'mymanifest' and archive all the
           files in the foo/ directory into 'classes.jar': 
       jar cvfm classes.jar mymanifest -C foo/ .

added manifest
adding: org/(in = 0) (out= 0)(stored 0%)
adding: org/hwone/(in = 0) (out= 0)(stored 0%)
adding: org/hwone/QueryAuthor$Reduce.class(in = 2168) (out= 959)(deflated 55%)
adding: org/hwone/QueryAuthor$Map.class(in = 2084) (out= 932)(deflated 55%)
adding: org/hwone/QueryAuthor.class(in = 2188) (out= 1085)(deflated 50%)
